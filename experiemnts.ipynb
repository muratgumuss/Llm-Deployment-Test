{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face x LangChain : A new partner package in LangChain\n",
    "langchain_huggingface, a partner package in LangChain jointly maintained by Hugging Face and LangChain. This new Python package is designed to bring the power of the latest development of Hugging Face into LangChain and keep it up to date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_huggingface in c:\\dev\\python\\.venv\\lib\\site-packages (0.3.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.65 in c:\\dev\\python\\.venv\\lib\\site-packages (from langchain_huggingface) (0.3.66)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in c:\\dev\\python\\.venv\\lib\\site-packages (from langchain_huggingface) (0.21.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.30.2 in c:\\dev\\python\\.venv\\lib\\site-packages (from langchain_huggingface) (0.33.1)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in c:\\dev\\python\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (0.3.45)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\dev\\python\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\dev\\python\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\dev\\python\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\dev\\python\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\dev\\python\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (4.13.2)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in c:\\dev\\python\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (2.11.7)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\dev\\python\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (3.0.0)\n",
      "Requirement already satisfied: filelock in c:\\dev\\python\\.venv\\lib\\site-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\dev\\python\\.venv\\lib\\site-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (2025.5.1)\n",
      "Requirement already satisfied: requests in c:\\dev\\python\\.venv\\lib\\site-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\dev\\python\\.venv\\lib\\site-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (4.67.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\dev\\python\\.venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\dev\\python\\.venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\dev\\python\\.venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\dev\\python\\.venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (0.23.0)\n",
      "Requirement already satisfied: anyio in c:\\dev\\python\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\dev\\python\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\dev\\python\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\dev\\python\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\dev\\python\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\dev\\python\\.venv\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\dev\\python\\.venv\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\dev\\python\\.venv\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\dev\\python\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.30.2->langchain_huggingface) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\dev\\python\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.30.2->langchain_huggingface) (2.4.0)\n",
      "Requirement already satisfied: colorama in c:\\dev\\python\\.venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.30.2->langchain_huggingface) (0.4.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\dev\\python\\.venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (1.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~angchain (C:\\dev\\Python\\.venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~angchain (C:\\dev\\Python\\.venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~angchain (C:\\dev\\Python\\.venv\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in c:\\dev\\python\\.venv\\lib\\site-packages (0.33.1)\n",
      "Requirement already satisfied: filelock in c:\\dev\\python\\.venv\\lib\\site-packages (from huggingface_hub) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\dev\\python\\.venv\\lib\\site-packages (from huggingface_hub) (2025.5.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\dev\\python\\.venv\\lib\\site-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\dev\\python\\.venv\\lib\\site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\dev\\python\\.venv\\lib\\site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\dev\\python\\.venv\\lib\\site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\dev\\python\\.venv\\lib\\site-packages (from huggingface_hub) (4.13.2)\n",
      "Requirement already satisfied: colorama in c:\\dev\\python\\.venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\dev\\python\\.venv\\lib\\site-packages (from requests->huggingface_hub) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\dev\\python\\.venv\\lib\\site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\dev\\python\\.venv\\lib\\site-packages (from requests->huggingface_hub) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\dev\\python\\.venv\\lib\\site-packages (from requests->huggingface_hub) (2025.6.15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~angchain (C:\\dev\\Python\\.venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~angchain (C:\\dev\\Python\\.venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~angchain (C:\\dev\\Python\\.venv\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "## API Call\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFaceEndpoint\n",
    "#### How to Access HuggingFace Models with API\n",
    "There are also two ways to use this class. You can specify the model with the repo_id parameter. Those endpoints use the serverless API, which is particularly beneficial to people using pro accounts or enterprise hub. Still, regular users can already have access to a fair amount of request by connecting with their HF token in the environment where they are executing the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.3', huggingfacehub_api_token='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', max_new_tokens=150, temperature=0.7, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='mistralai/Mistral-7B-Instruct-v0.3', client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,max_new_tokens=150,temperature=0.7)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Model mistralai/Mistral-7B-Instruct-v0.3 is not supported for task text-generation and provider novita. Supported task: conversational.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is machine learning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\Python\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:389\u001b[39m, in \u001b[36mBaseLLM.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    380\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    385\u001b[39m     **kwargs: Any,\n\u001b[32m    386\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    387\u001b[39m     config = ensure_config(config)\n\u001b[32m    388\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    399\u001b[39m         .generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m    400\u001b[39m         .text\n\u001b[32m    401\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\Python\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:766\u001b[39m, in \u001b[36mBaseLLM.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    757\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    758\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    759\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    763\u001b[39m     **kwargs: Any,\n\u001b[32m    764\u001b[39m ) -> LLMResult:\n\u001b[32m    765\u001b[39m     prompt_strings = [p.to_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m766\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\Python\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:973\u001b[39m, in \u001b[36mBaseLLM.generate\u001b[39m\u001b[34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    958\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m    959\u001b[39m     run_managers = [\n\u001b[32m    960\u001b[39m         callback_manager.on_llm_start(\n\u001b[32m    961\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m    971\u001b[39m         )\n\u001b[32m    972\u001b[39m     ]\n\u001b[32m--> \u001b[39m\u001b[32m973\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) > \u001b[32m0\u001b[39m:\n\u001b[32m    981\u001b[39m     run_managers = [\n\u001b[32m    982\u001b[39m         callback_managers[idx].on_llm_start(\n\u001b[32m    983\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m    990\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m missing_prompt_idxs\n\u001b[32m    991\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\Python\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:792\u001b[39m, in \u001b[36mBaseLLM._generate_helper\u001b[39m\u001b[34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[39m\n\u001b[32m    781\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate_helper\u001b[39m(\n\u001b[32m    782\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    783\u001b[39m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    788\u001b[39m     **kwargs: Any,\n\u001b[32m    789\u001b[39m ) -> LLMResult:\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    791\u001b[39m         output = (\n\u001b[32m--> \u001b[39m\u001b[32m792\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[32m    796\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    799\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    800\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._generate(prompts, stop=stop)\n\u001b[32m    801\u001b[39m         )\n\u001b[32m    802\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    803\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\Python\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:1547\u001b[39m, in \u001b[36mLLM._generate\u001b[39m\u001b[34m(self, prompts, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1544\u001b[39m new_arg_supported = inspect.signature(\u001b[38;5;28mself\u001b[39m._call).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1545\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[32m   1546\u001b[39m     text = (\n\u001b[32m-> \u001b[39m\u001b[32m1547\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1548\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m   1549\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(prompt, stop=stop, **kwargs)\n\u001b[32m   1550\u001b[39m     )\n\u001b[32m   1551\u001b[39m     generations.append([Generation(text=text)])\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations=generations)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\Python\\.venv\\Lib\\site-packages\\langchain_huggingface\\llms\\huggingface_endpoint.py:312\u001b[39m, in \u001b[36mHuggingFaceEndpoint._call\u001b[39m\u001b[34m(self, prompt, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m completion\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m     response_text = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minvocation_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m     \u001b[38;5;66;03m# Maybe the generation has stopped at one of the stop sequences:\u001b[39;00m\n\u001b[32m    319\u001b[39m     \u001b[38;5;66;03m# then we remove this stop sequence from the end of the generated text\u001b[39;00m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m stop_seq \u001b[38;5;129;01min\u001b[39;00m invocation_params[\u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\Python\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2297\u001b[39m, in \u001b[36mInferenceClient.text_generation\u001b[39m\u001b[34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001b[39m\n\u001b[32m   2295\u001b[39m model_id = model \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model\n\u001b[32m   2296\u001b[39m provider_helper = get_provider_helper(\u001b[38;5;28mself\u001b[39m.provider, task=\u001b[33m\"\u001b[39m\u001b[33mtext-generation\u001b[39m\u001b[33m\"\u001b[39m, model=model_id)\n\u001b[32m-> \u001b[39m\u001b[32m2297\u001b[39m request_parameters = \u001b[43mprovider_helper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprepare_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2298\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2300\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_payload\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2301\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2302\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2303\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2304\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2306\u001b[39m \u001b[38;5;66;03m# Handle errors separately for more precise error messages\u001b[39;00m\n\u001b[32m   2307\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\Python\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_providers\\_common.py:93\u001b[39m, in \u001b[36mTaskProviderHelper.prepare_request\u001b[39m\u001b[34m(self, inputs, parameters, headers, model, api_key, extra_payload)\u001b[39m\n\u001b[32m     90\u001b[39m api_key = \u001b[38;5;28mself\u001b[39m._prepare_api_key(api_key)\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# mapped model from HF model ID\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m provider_mapping_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_mapping_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# default HF headers + user headers (to customize in subclasses)\u001b[39;00m\n\u001b[32m     96\u001b[39m headers = \u001b[38;5;28mself\u001b[39m._prepare_headers(headers, api_key)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\Python\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_providers\\_common.py:162\u001b[39m, in \u001b[36mTaskProviderHelper._prepare_mapping_info\u001b[39m\u001b[34m(self, model)\u001b[39m\n\u001b[32m    159\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not supported by provider \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m provider_mapping.task != \u001b[38;5;28mself\u001b[39m.task:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    163\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not supported for task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.task\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and provider \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    164\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSupported task: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprovider_mapping.task\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    165\u001b[39m     )\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m provider_mapping.status == \u001b[33m\"\u001b[39m\u001b[33mstaging\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    168\u001b[39m     logger.warning(\n\u001b[32m    169\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is in staging mode for provider \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Meant for test purposes only.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    170\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Model mistralai/Mistral-7B-Instruct-v0.3 is not supported for task text-generation and provider novita. Supported task: conversational."
     ]
    }
   ],
   "source": [
    "llm.invoke(\"What is machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'❓\\n\\nGenerative AI refers to a type of artificial intelligence that can create new content, such as images, music, or text, based on patterns learned from existing data. Generative AI models use techniques such as deep learning, neural networks, and reinforcement learning to generate new content that is similar to, but not identical to, the data they were trained on. Some examples of generative AI include deepfakes, AI-generated art, and AI-written stories.\\n\\nGenerative AI has a wide range of potential applications, including creating personalized content for users, generating new ideas for designers and creators, and automating tasks such as data entry and content creation. However, generative AI also raises ethical concerns, such as the potential for misuse in creating deepfakes or generating offensive or harmful content. It is important for developers and users of generative AI to be aware of these potential risks and to take steps to mitigate them.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is generative AI \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='google/gemma-2-9b', huggingfacehub_api_token='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', max_new_tokens=150, temperature=0.7, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='google/gemma-2-9b', client=<InferenceClient(model='google/gemma-2-9b', timeout=120)>, async_client=<InferenceClient(model='google/gemma-2-9b', timeout=120)>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id=\"google/gemma-2-9b\"\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,max_new_tokens=150,temperature=0.7)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'?\\n\\nMachine learning is a method of data analysis that automates analytical model building. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention.\\n\\nIt is a subset of artificial intelligence that enables computers to learn without being explicitly programmed. Machine learning focuses on the development of computer programs that can access data and use it to learn for themselves.\\n\\nMachine learning algorithms use computational methods to “learn” information directly from data without relying on a predetermined equation as a model. The algorithms adapt to the data, looking for patterns and building a model from examples in order to make predictions or decisions without being explicitly programmed to do so.\\n\\nMachine learning algorithms are used in a wide variety'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='google/gemma-2-9b', huggingfacehub_api_token='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', max_new_tokens=150, temperature=0.7, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='google/gemma-2-9b', client=<InferenceClient(model='google/gemma-2-9b', timeout=120)>, async_client=<InferenceClient(model='google/gemma-2-9b', timeout=120)>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id=\"google/gemma-2-9b\"\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,max_new_tokens=150,temperature=0.7)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['question'] input_types={} partial_variables={} template='\\nQuestion:{question}\\nAnswer:Lets think step by step.\\n'\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate,LLMChain\n",
    "template=\"\"\"\n",
    "Question:{question}\n",
    "Answer:Lets think step by step.\n",
    "\"\"\"\n",
    "prompt=PromptTemplate(template=template,input_variables=[\"question\"])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'?\\n\\nWho won Turkish Super League on 2024?\\n\\nWho won Turkish Super League on 2023?\\n\\nWho won Turkish Super League on 2023?\\n\\nFenerbahçe has won the Turkish Super League title for the 2022-23 season with a 2-0 victory over Konyaspor at the Başakşehir Fatih Terim Stadium.\\n\\nWho won Turkish Super League on 2024?\\n\\nWho won Turkish Super League on 2024?\\n\\nThe Turkish Super League, or Süper Lig, is a professional football league in Turkey. It is the highest level of the football league system in Turkey, and is contested by 20 teams'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain=LLMChain(llm=llm,prompt=prompt)\n",
    "llm.invoke(\"Who won Turkish Super League on 2024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\murat\\AppData\\Local\\Temp\\ipykernel_96452\\2593296450.py:6: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  hf = HuggingFaceBgeEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\dev\\Python\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e0979e9bf044818d6ea949b4d3c369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc206e36f030455ab97ffb20b41e32d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a812698bb5f44e8c9f57747972c13bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1895bef82d3432199be9a2ca8047381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93b909d5c7344f3aa435019d3b1cd670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff91a50e087645cc9658519781c3d3ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c5f57e2d814a80b6a9853c55628dea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba928d2e578c40c4b9a25ac0de075102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3990782f1f6e4b139add0c983813e062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a50a3487bef4b0da34e63aeed9ed462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06a85fe0a7294b33a30b98bcf08aba4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = hf.embed_query(\"hi this is harrison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.028416533023118973,\n",
       " 0.012183255515992641,\n",
       " 0.02744399383664131,\n",
       " -0.05482873320579529,\n",
       " 0.024238908663392067,\n",
       " 0.0007662165444344282,\n",
       " 0.06783363968133926,\n",
       " 0.016348320990800858,\n",
       " -0.018950749188661575,\n",
       " 0.01254288014024496,\n",
       " 0.02156497910618782,\n",
       " -0.08793042600154877,\n",
       " 0.0006460649892687798,\n",
       " 0.033270783722400665,\n",
       " 0.005463775247335434,\n",
       " -0.060376398265361786,\n",
       " 0.05042261630296707,\n",
       " 0.004434782546013594,\n",
       " 0.0009598929900676012,\n",
       " 0.0017405434045940638,\n",
       " 0.003298830706626177,\n",
       " 0.03167252615094185,\n",
       " -0.04880751296877861,\n",
       " -0.04481912776827812,\n",
       " 0.07132109999656677,\n",
       " -0.007510832976549864,\n",
       " -0.0011259512975811958,\n",
       " -0.015801167115569115,\n",
       " -0.029402388259768486,\n",
       " -0.17224563658237457,\n",
       " -0.03189516440033913,\n",
       " -0.0016291390638798475,\n",
       " 0.018105005845427513,\n",
       " 0.015315379947423935,\n",
       " -0.020729564130306244,\n",
       " -0.008873016573488712,\n",
       " -0.001282249460928142,\n",
       " 0.027276940643787384,\n",
       " -0.010114269331097603,\n",
       " 0.012621641159057617,\n",
       " -0.0070778606459498405,\n",
       " -0.01669318974018097,\n",
       " 0.040855810046195984,\n",
       " 0.023938367143273354,\n",
       " -0.02008153311908245,\n",
       " 0.028681136667728424,\n",
       " -0.01940072700381279,\n",
       " -0.014618167653679848,\n",
       " 0.017379673197865486,\n",
       " 0.004164119251072407,\n",
       " 0.06415648758411407,\n",
       " 0.047683071345090866,\n",
       " 0.0018365175928920507,\n",
       " -8.065404108492658e-05,\n",
       " 0.016596809029579163,\n",
       " 0.011124222539365292,\n",
       " 0.0696944072842598,\n",
       " 0.051820553839206696,\n",
       " 0.05568530783057213,\n",
       " 0.05551541596651077,\n",
       " 0.000503936258610338,\n",
       " 0.0418705977499485,\n",
       " -0.15344087779521942,\n",
       " 0.051807861775159836,\n",
       " 0.006689776200801134,\n",
       " -0.03167067468166351,\n",
       " -0.00910498108714819,\n",
       " -0.051604729145765305,\n",
       " 0.042508576065301895,\n",
       " 0.02820001170039177,\n",
       " -0.01074815820902586,\n",
       " 0.022405780851840973,\n",
       " 0.04439551383256912,\n",
       " 0.00411552470177412,\n",
       " 0.018998444080352783,\n",
       " -0.004357198253273964,\n",
       " 0.04762765020132065,\n",
       " 0.01182466372847557,\n",
       " 0.008164619095623493,\n",
       " 0.008177274838089943,\n",
       " -0.009698772802948952,\n",
       " -0.01426023244857788,\n",
       " 0.011409698985517025,\n",
       " -0.07362114638090134,\n",
       " -0.054395172744989395,\n",
       " -0.057039618492126465,\n",
       " -0.0036085352767258883,\n",
       " 0.002666106913238764,\n",
       " 0.023782474920153618,\n",
       " 0.015376237221062183,\n",
       " -0.07020365446805954,\n",
       " -0.03130035474896431,\n",
       " -0.0031142588704824448,\n",
       " -0.015812186524271965,\n",
       " -0.03791400417685509,\n",
       " -0.025921931490302086,\n",
       " 0.018168430775403976,\n",
       " -0.03882451355457306,\n",
       " -0.056745100766420364,\n",
       " 0.5792059302330017,\n",
       " -0.05278836190700531,\n",
       " 0.020716357976198196,\n",
       " 0.06794385612010956,\n",
       " -0.04541650041937828,\n",
       " 0.011642497964203358,\n",
       " -0.021571746096014977,\n",
       " 0.02034168876707554,\n",
       " -0.027448924258351326,\n",
       " -0.04558899626135826,\n",
       " -0.02944358065724373,\n",
       " -0.023662494495511055,\n",
       " -0.03431522473692894,\n",
       " 0.0019387906650081277,\n",
       " -0.07095134258270264,\n",
       " 0.034556351602077484,\n",
       " -0.030558930709958076,\n",
       " 0.03907856345176697,\n",
       " -0.02970733493566513,\n",
       " -0.0008283042698167264,\n",
       " -0.012159377336502075,\n",
       " -0.01827283762395382,\n",
       " 0.025486506521701813,\n",
       " -0.004461678210645914,\n",
       " 0.016335299238562584,\n",
       " 0.019126467406749725,\n",
       " -0.05483204871416092,\n",
       " 0.02763594128191471,\n",
       " -0.004757631570100784,\n",
       " 0.059001706540584564,\n",
       " -0.0016944793751463294,\n",
       " 0.008014945313334465,\n",
       " -0.03772684186697006,\n",
       " -0.09893050044775009,\n",
       " -0.022574380040168762,\n",
       " -0.037604693323373795,\n",
       " -0.0021698700729757547,\n",
       " 0.0032446132972836494,\n",
       " -0.019202541559934616,\n",
       " -0.00863118376582861,\n",
       " -0.04802306741476059,\n",
       " 0.00869668647646904,\n",
       " -0.09516111016273499,\n",
       " -0.03496043384075165,\n",
       " -0.04360796511173248,\n",
       " -0.00034398966818116605,\n",
       " -0.010173669084906578,\n",
       " -0.03099953755736351,\n",
       " 0.02430965006351471,\n",
       " -0.020402055233716965,\n",
       " 0.031139392405748367,\n",
       " 0.00088112847879529,\n",
       " 0.01391651388257742,\n",
       " -0.031196244060993195,\n",
       " -0.037154003977775574,\n",
       " 0.004029604606330395,\n",
       " 0.014799808152019978,\n",
       " 0.04318896681070328,\n",
       " 0.03875479847192764,\n",
       " 0.01385202445089817,\n",
       " 0.01979784667491913,\n",
       " 0.01026705652475357,\n",
       " -0.00543412659317255,\n",
       " -0.014299213886260986,\n",
       " 0.027637839317321777,\n",
       " 0.009802649728953838,\n",
       " -0.1355028599500656,\n",
       " -0.01713971607387066,\n",
       " 0.017617100849747658,\n",
       " 0.02313223108649254,\n",
       " 0.0017589469207450747,\n",
       " 0.030889451503753662,\n",
       " 0.0399186909198761,\n",
       " -0.013684157282114029,\n",
       " 0.024816496297717094,\n",
       " 0.05405019223690033,\n",
       " 0.017761169001460075,\n",
       " -0.01847507618367672,\n",
       " 0.02595536969602108,\n",
       " -0.006377554032951593,\n",
       " -0.016587305814027786,\n",
       " 0.037848036736249924,\n",
       " -0.027290068566799164,\n",
       " -0.052845824509859085,\n",
       " -0.03803316503763199,\n",
       " 0.05191108211874962,\n",
       " -0.00755709782242775,\n",
       " -0.03180532902479172,\n",
       " 0.013284171000123024,\n",
       " -0.02772371657192707,\n",
       " 0.05630652233958244,\n",
       " 0.0030418557580560446,\n",
       " 0.05332481488585472,\n",
       " -0.057911261916160583,\n",
       " -0.011325842700898647,\n",
       " -0.031172025948762894,\n",
       " 0.02560868300497532,\n",
       " 0.03389060124754906,\n",
       " -0.0010285027092322707,\n",
       " 0.01586487889289856,\n",
       " 0.010595225729048252,\n",
       " -0.02703779563307762,\n",
       " -0.0009308372973464429,\n",
       " -0.048152223229408264,\n",
       " 0.02817923203110695,\n",
       " 0.010320615023374557,\n",
       " 0.06662958115339279,\n",
       " -0.016558170318603516,\n",
       " -0.004431346897035837,\n",
       " 0.03823428601026535,\n",
       " -0.023408176377415657,\n",
       " -0.03558174893260002,\n",
       " -0.058290716260671616,\n",
       " -0.011181495152413845,\n",
       " -0.017684578895568848,\n",
       " -0.016141291707754135,\n",
       " -0.03424535319209099,\n",
       " -0.02513953298330307,\n",
       " 0.039396677166223526,\n",
       " -0.023658188059926033,\n",
       " -0.007725039962679148,\n",
       " -0.005098922643810511,\n",
       " -0.03523433580994606,\n",
       " -0.014076863415539265,\n",
       " -0.22326034307479858,\n",
       " -0.031471338123083115,\n",
       " -0.0012906108750030398,\n",
       " -0.0017200120491907,\n",
       " -0.007846081629395485,\n",
       " -0.05802324041724205,\n",
       " 0.0461745411157608,\n",
       " 0.024552665650844574,\n",
       " 0.07320841401815414,\n",
       " 0.017268339172005653,\n",
       " 0.04761209338903427,\n",
       " 0.013473328202962875,\n",
       " -0.005516061093658209,\n",
       " -0.014357831329107285,\n",
       " -0.009674303233623505,\n",
       " 0.04878249391913414,\n",
       " 0.03053806908428669,\n",
       " -0.02499397099018097,\n",
       " 0.021486256271600723,\n",
       " 0.01763986051082611,\n",
       " 0.05313888192176819,\n",
       " 0.013485022820532322,\n",
       " -0.023225992918014526,\n",
       " -0.021403999999165535,\n",
       " 0.026075368747115135,\n",
       " 0.0020292047411203384,\n",
       " 0.12753745913505554,\n",
       " 0.08316841721534729,\n",
       " 0.044089484959840775,\n",
       " -0.026703588664531708,\n",
       " 0.005521971732378006,\n",
       " -0.009294900111854076,\n",
       " 0.020074255764484406,\n",
       " -0.09684179723262787,\n",
       " -0.02470395341515541,\n",
       " 0.025086993351578712,\n",
       " 0.00208862847648561,\n",
       " -0.044894054532051086,\n",
       " -0.07861136645078659,\n",
       " -0.004376356024295092,\n",
       " -0.06590455025434494,\n",
       " 0.014689392410218716,\n",
       " -0.057641834020614624,\n",
       " -0.07152022421360016,\n",
       " -0.06232651323080063,\n",
       " 0.0034316396340727806,\n",
       " -0.04606549069285393,\n",
       " 0.04530089721083641,\n",
       " -0.026762302964925766,\n",
       " 0.03401092067360878,\n",
       " 0.04547378420829773,\n",
       " -0.02817920595407486,\n",
       " 0.0050117941573262215,\n",
       " 0.009630816988646984,\n",
       " -0.03030562587082386,\n",
       " -0.03612489998340607,\n",
       " -0.013627007603645325,\n",
       " -0.03265371918678284,\n",
       " -0.04467753693461418,\n",
       " 0.01064220443367958,\n",
       " -0.027486318722367287,\n",
       " -0.024565115571022034,\n",
       " -0.024747760966420174,\n",
       " 0.05361958593130112,\n",
       " 0.020789915695786476,\n",
       " 0.019468458369374275,\n",
       " 0.05324116721749306,\n",
       " -0.01400243490934372,\n",
       " 0.021243242546916008,\n",
       " -0.04957326874136925,\n",
       " -0.00852258037775755,\n",
       " 0.007852889597415924,\n",
       " -0.05719393119215965,\n",
       " -0.027550645172595978,\n",
       " 0.005300881806761026,\n",
       " 0.04007292538881302,\n",
       " 0.01959792897105217,\n",
       " -0.04519733414053917,\n",
       " 0.03243579715490341,\n",
       " -0.012342470698058605,\n",
       " 0.03431437164545059,\n",
       " 0.02110210619866848,\n",
       " 0.0398465171456337,\n",
       " 0.03166376054286957,\n",
       " -0.03359024226665497,\n",
       " 0.03164789080619812,\n",
       " -0.0033045082818716764,\n",
       " 0.004641883075237274,\n",
       " 0.03758940100669861,\n",
       " -0.059244561940431595,\n",
       " 0.0070283482782542706,\n",
       " 0.0038086979184299707,\n",
       " -0.02578887902200222,\n",
       " -0.021203365176916122,\n",
       " 0.02269119583070278,\n",
       " -0.02177293598651886,\n",
       " -0.2796378433704376,\n",
       " 0.007267358247190714,\n",
       " 0.02107202261686325,\n",
       " 0.0451974980533123,\n",
       " -0.02053447626531124,\n",
       " 0.024313699454069138,\n",
       " -0.0006136376177892089,\n",
       " -0.011857058852910995,\n",
       " -0.03296777233481407,\n",
       " 0.035843200981616974,\n",
       " 0.031281713396310806,\n",
       " 0.06373961269855499,\n",
       " 0.046547845005989075,\n",
       " -0.014470532536506653,\n",
       " 0.015869759023189545,\n",
       " 0.033971283584833145,\n",
       " 0.018059611320495605,\n",
       " 0.0022987606935203075,\n",
       " 0.016549857333302498,\n",
       " -0.021714871749281883,\n",
       " -0.03485998138785362,\n",
       " -0.0008649206720292568,\n",
       " 0.15126049518585205,\n",
       " -0.02453676238656044,\n",
       " 0.030671244487166405,\n",
       " -0.007318182848393917,\n",
       " -0.006135463248938322,\n",
       " 0.06415151059627533,\n",
       " 0.01602148823440075,\n",
       " -0.03636440262198448,\n",
       " 0.019898591563105583,\n",
       " -0.02117234095931053,\n",
       " 0.048294104635715485,\n",
       " -0.044780973345041275,\n",
       " 0.0476338192820549,\n",
       " 0.0007749323267489672,\n",
       " -0.005927944555878639,\n",
       " 0.061542633920907974,\n",
       " 0.023968420922756195,\n",
       " 0.013304989784955978,\n",
       " 0.022684557363390923,\n",
       " 0.014538057148456573,\n",
       " -0.05215907469391823,\n",
       " -0.03274961933493614,\n",
       " 0.08583347499370575,\n",
       " -0.0037247873842716217,\n",
       " 0.0013494155136868358,\n",
       " 0.04091983661055565,\n",
       " 0.011659671552479267,\n",
       " 0.05843616649508476,\n",
       " -0.022286169230937958,\n",
       " -0.011520696803927422,\n",
       " 0.004705706145614386,\n",
       " 0.047182634472846985,\n",
       " -0.0019179105293005705,\n",
       " 0.0330093577504158,\n",
       " -0.03505057841539383,\n",
       " -0.0207365769892931,\n",
       " -0.009222167544066906,\n",
       " 0.014618238434195518,\n",
       " 0.006456065457314253,\n",
       " 0.0010978200007230043,\n",
       " 0.010224022902548313,\n",
       " 0.08537213504314423,\n",
       " 0.03883952647447586]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
