#1. Gerekli KÃ¼tÃ¼phanelerin YÃ¼klenmesi
"""
validators: Girilen URL'nin geÃ§erli olup olmadÄ±ÄŸÄ±nÄ± kontrol eder.

streamlit: Web uygulamasÄ± oluÅŸturmak iÃ§in kullanÄ±lan framework.

langchain.prompts.PromptTemplate: LLM iÃ§in Ã¶zel prompt ÅŸablonu tanÄ±mlamak iÃ§in.

ChatGroq: Groqâ€™un LLM APIâ€™sine eriÅŸim saÄŸlayan sÄ±nÄ±f.

load_summarize_chain: Belge Ã¶zetleme zinciri kurmak iÃ§in kullanÄ±lÄ±r.

UnstructuredURLLoader: URL'den veri Ã§eken LangChain bileÅŸeni (kullanÄ±lmamÄ±ÅŸ).

YouTubeTranscriptApi: YouTube videolarÄ±ndan otomatik altyazÄ± almak iÃ§in.

Document: LangChain iÃ§eriÄŸini taÅŸÄ±mak iÃ§in kullanÄ±lan temel yapÄ±.

requests, BeautifulSoup: Web sayfalarÄ±nÄ±n iÃ§eriÄŸini Ã§ekmek ve analiz etmek iÃ§in.
"""

import validators
import streamlit as st
from langchain.prompts import PromptTemplate
from langchain_groq import ChatGroq
from langchain.chains.summarize import load_summarize_chain
from langchain_community.document_loaders import UnstructuredURLLoader
from youtube_transcript_api import YouTubeTranscriptApi
from langchain_core.documents import Document
import requests
from bs4 import BeautifulSoup
import time
import random

#ğŸ› 2. ArayÃ¼z YapÄ±landÄ±rmasÄ±
"""
Streamlit sayfasÄ±nÄ±n baÅŸlÄ±ÄŸÄ±nÄ± ve gÃ¶rÃ¼nen baÅŸlÄ±k metnini tanÄ±mlar.
"""
# Streamlit arayÃ¼zÃ¼
st.set_page_config(page_title="Advanced Content Summarizer")
st.title("Smart Content Summarizer")
st.subheader("Enter URL to summarize")

#3. API Anahtar GiriÅŸi (Sidebar'da)
"""
KullanÄ±cÄ±nÄ±n Groq API anahtarÄ±nÄ± girmesi iÃ§in bir parola alanÄ± oluÅŸturur.

Uzun iÃ§eriklerin parÃ§alanarak Ã¶zetlenebileceÄŸi bilgisi verilir.
"""
# Sidebar for API key
with st.sidebar:
    groq_api_key = st.text_input("Groq API Key", type="password")
    st.info("For longer content, summaries may be chunked")

# 4. URL Girdisi
"""
KullanÄ±cÄ±nÄ±n Ã¶zetlemek istediÄŸi URLâ€™yi girmesi iÃ§in bir metin kutusu.
"""
url_input = st.text_input("URL", label_visibility="collapsed")

#5. Prompt Åablonu TanÄ±mÄ±
"""
LLMâ€™e verilecek girdinin nasÄ±l yapÄ±landÄ±rÄ±lacaÄŸÄ±nÄ± tanÄ±mlar.

{text} kÄ±smÄ± dinamik olarak iÃ§erikle doldurulur.
"""
# Prompt template
prompt_template = """
Please provide a concise summary of the following content in about 300 words.
Focus on key points and main ideas:

Content:{text}

SUMMARY:
"""
prompt = PromptTemplate(template=prompt_template, input_variables=["text"])

# 6. LLM Nesnesi OluÅŸturma
"""
Groq Ã¼zerinden LLaMA 3 modelini Ã§alÄ±ÅŸtÄ±racak LLM nesnesi oluÅŸturur.

temperature=0.3 â†’ daha tutarlÄ±, az yaratÄ±cÄ± Ã§Ä±ktÄ± saÄŸlar.
"""

# Optimized LLM initialization
def get_llm():
    return ChatGroq(
        model_name="llama3-8b-8192",
        groq_api_key=groq_api_key,
        temperature=0.3,
        max_tokens=1024
    )

#7. Web Ä°Ã§eriÄŸi Ã‡ekme Fonksiyonu
"""
requests ile URLâ€™ye istek gÃ¶nderir.

BeautifulSoup ile HTML parse edilir.

article, main, div.content gibi ana iÃ§erik bloklarÄ±nÄ± arar.

Bulamazsa body iÃ§eriÄŸinin ilk 5000 karakterini dÃ¶ndÃ¼rÃ¼r.

Temizleme iÅŸlemi ile script, style gibi gereksiz elemanlar Ã§Ä±karÄ±lÄ±r.
"""
# Improved web content extractor
def get_web_content(url):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept-Language': 'en-US,en;q=0.5'
        }
        
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Remove unwanted elements
        for element in soup(['script', 'style', 'nav', 'footer', 'iframe', 'noscript']):
            element.decompose()
        
        # Try to find main content
        for tag in ['article', 'main', 'div.content', 'div.post-content']:
            content = soup.find(tag)
            if content:
                text = content.get_text(separator='\n', strip=True)
                if len(text) > 200:
                    return Document(page_content=text)
        
        # Fallback to body
        text = soup.get_text(separator='\n', strip=True)
        return Document(page_content=text[:5000])  # Limit content size
    
    except Exception as e:
        raise RuntimeError(f"Could not fetch web content: {str(e)}")

#8. YouTube Transcript Ã‡ekme Fonksiyonu
"""
YouTube linkinden video_id Ã§Ä±karÄ±lÄ±r.

YouTubeTranscriptApi.get_transcript Ã§aÄŸrÄ±sÄ± yapÄ±lÄ±r.

AltyazÄ±lar birleÅŸtirilir.

Hata olursa baÅŸlÄ±k ve aÃ§Ä±klama gibi fallback metadata Ã§ekilir.
"""
# YouTube transcript fetcher
def get_youtube_transcript(video_url):
    try:
        video_id = video_url.split("v=")[-1].split("&")[0] if "v=" in video_url else video_url.split("youtu.be/")[-1].split("?")[0]
        
        try:
            transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en', 'tr'])
            text = " ".join([entry['text'] for entry in transcript])
            return Document(page_content=text[:5000])  # Limit transcript size
        except:
            # Fallback to metadata
            response = requests.get(video_url, headers={'User-Agent': 'Mozilla/5.0'})
            soup = BeautifulSoup(response.text, 'html.parser')
            title = soup.find('meta', property='og:title')
            description = soup.find('meta', property='og:description')
            content = f"Title: {title['content'] if title else 'No title'}\n\nDescription: {description['content'] if description else 'No description'}"
            return Document(page_content=content)
            
    except Exception as e:
        raise RuntimeError(f"YouTube error: {str(e)}")

#9. Uzun Ä°Ã§erikler Ä°Ã§in ParÃ§alÄ± Ã–zetleme
"""
2000 karakterlik parÃ§alara ayÄ±rÄ±r.

Her parÃ§ayÄ± load_summarize_chain ile Ã¶zetler.

Aralara time.sleep(1) koyarak rate-limit riskini azaltÄ±r.

Ã–zetleri birleÅŸtirip tek bir Ã§Ä±ktÄ± olarak dÃ¶ndÃ¼rÃ¼r.
"""
# Chunked summarization
def summarize_large_content(docs):
    llm = get_llm()
    
    # Split large content
    content = docs[0].page_content
    chunks = [content[i:i+2000] for i in range(0, len(content), 2000)]
    
    # Process each chunk
    summaries = []
    progress_bar = st.progress(0)
    
    for i, chunk in enumerate(chunks):
        progress_bar.progress((i + 1) / len(chunks))
        chunk_doc = Document(page_content=chunk)
        chain = load_summarize_chain(llm, chain_type="stuff", prompt=prompt)
        summary = chain.run([chunk_doc])
        summaries.append(summary)
        time.sleep(1)  # Rate limit control
    
    return "\n\n".join(summaries)
#10. URL Ä°ÅŸleme (Tetikleme Fonksiyonu)
"""
YouTube ise get_youtube_transcript, deÄŸilse get_web_content Ã§aÄŸÄ±rÄ±lÄ±r.

Ä°Ã§erik uzun ise parÃ§alayÄ±p Ã¶zetler, deÄŸilse direkt Ã¶zet zinciri Ã§alÄ±ÅŸtÄ±rÄ±r.

Hata oluÅŸursa yakalanÄ±r ve kullanÄ±cÄ±ya bildirilir.
"""

# Main processing function
def process_url(url):
    try:
        with st.spinner("Extracting content..."):
            if "youtube.com" in url or "youtu.be" in url:
                docs = [get_youtube_transcript(url)]
            else:
                docs = [get_web_content(url)]
                
            if not docs or not docs[0].page_content.strip():
                raise ValueError("No content found")
            
            with st.spinner("Summarizing..."):
                if len(docs[0].page_content) > 2000:
                    return summarize_large_content(docs)
                else:
                    chain = load_summarize_chain(get_llm(), chain_type="stuff", prompt=prompt)
                    return chain.run(docs)
                    
    except Exception as e:
        st.error(f"Processing error: {str(e)}")
        raise
#11. UI â€“ â€œSummarizeâ€ Butonunun DavranÄ±ÅŸÄ±
"""
KullanÄ±cÄ± butona bastÄ±ÄŸÄ±nda tetiklenir.

API anahtarÄ± veya URL eksikse hata verilir.

Ã–zetleme yapÄ±lÄ±r, sonuÃ§ kullanÄ±cÄ±ya sunulur.

Ek olarak, orijinal iÃ§erik â€œView extracted contentâ€ alanÄ±nda gÃ¶sterilir.
"""
# Streamlit UI handler
if st.button("Summarize Content"):
    if not groq_api_key.strip():
        st.error("Please enter your Groq API key")
    elif not validators.url(url_input):
        st.error("Please enter a valid URL")
    else:
        try:
            result = process_url(url_input)
            st.subheader("Summary")
            st.success(result)
            
            with st.expander("View extracted content"):
                st.text_area("Content", get_web_content(url_input).page_content[:5000], height=300)
                
        except Exception as e:
            st.error(f"Failed to summarize: {str(e)}")